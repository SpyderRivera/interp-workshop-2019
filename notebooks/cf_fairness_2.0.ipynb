{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComputeFest 2019\n",
    "# Model Agnostic Methods for Interpretability and Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# AIF360 imports\n",
    "from aif360.algorithms.preprocessing import OptimPreproc\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions import get_distortion_adult\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Income dataset\n",
    "\n",
    "The previous dataset was thorough and complex enough to demonstrate interpretability techniques, but as it is an anonymized dataset, it has little to no information on sensitive features. We will switch to another dataset for this part that is more suited to analyzing fairness techniques, as it possesses information on gender and race. \n",
    "\n",
    "This dataset is called the **Census Income dataset**, and it associates features of working adults to **whether or not they make more than $50k/yr**. It is extracted from the 1994 Census database, and contains **48842 observations** with a mix of continuous and categorical features (14 in total).  \n",
    "\n",
    "List of features:\n",
    "- **age:** continuous. \n",
    "- **workclass:** categorical. \n",
    "- **education:** categorical. \n",
    "- **education-num:** continuous. \n",
    "- **marital-status:** categorical. \n",
    "- **relationship:** categorical. \n",
    "- **race:** categorical. \n",
    "- **sex:** categorical. \n",
    "- **capital-gain:** continuous. \n",
    "- **capital-loss:** continuous. \n",
    "- **hours-per-week:** continuous. \n",
    "- **fnlwgt:** (final weight) continuous. \n",
    "- **native-country:** categorical.\n",
    "\n",
    "Response: binary, corresponding to >50K (1) or <=50K (0). \n",
    "\n",
    "\n",
    "#### Reference:\n",
    "Ron Kohavi, \"Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid\", Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we import it?\n",
    "\n",
    "To import this dataset in an easy way, we will use a very convenient module (that will be presented in detail later this afternoon!) called AIF360, created by IBM. It centralizes multiple fairness metrics and tools for training fair models, as well as easy ways to import useful datasets. We will be using it to import our dataset and ultimately create a fair model, but we will implement our own fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Census Income Dataset from AIF360\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "dataset_orig = load_preproc_data_adult(['sex'])\n",
    "optim_options = {\n",
    "    \"distortion_fun\": get_distortion_adult,\n",
    "    \"epsilon\": 0.05,\n",
    "    \"clist\": [0.99, 1.99, 2.99],\n",
    "    \"dlist\": [.1, 0.05, 0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print some characteristics of our dataset to see what we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset shape: (34189, 18) \n",
      "\n",
      "Protected attribute names: ['sex'] \n",
      "\n",
      "Privileged and unprivileged protected attribute values: [array([1.])] [array([0.])] \n",
      "\n",
      "Dataset feature names: ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training Dataset shape:',dataset_orig_train.features.shape,'\\n')\n",
    "# print('Favorable and unfavorable labels:',dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "print('Protected attribute names:',dataset_orig_train.protected_attribute_names,'\\n')\n",
    "print('Privileged and unprivileged protected attribute values:' ,dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes,'\\n')\n",
    "print('Dataset feature names:',dataset_orig_train.feature_names,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we didn't import the exact dataset described above, we actually imported a slightly modified version with only binary features that are easier to understand and work with.\n",
    "\n",
    "We now split the data into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we then extract numpy arrays from the dataset objects given by AIF360, standardizing the features in the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numpy arrays for x_train, x_test, y_train, y_test by extracting data from AIF360 dataset object\n",
    "\n",
    "# We define a scaler to normalize our data\n",
    "scale_orig = StandardScaler()\n",
    "\n",
    "# We get our training numpy arrays\n",
    "x_train = scale_orig.fit_transform(dataset_orig_train.features) #This fit_transform from the scaler substracts mean and divides by std for each feature.\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "# And our testing arrays\n",
    "x_test = scale_orig.transform(dataset_orig_test.features) # Here, we only transform, as we can't use the testing set to define the scaling factors.\n",
    "y_test = dataset_orig_test.labels.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our classifier (Random Forest)\n",
    "\n",
    "It is now time to train the classifier that we are going to Audit. We chose a RandomForest here for it's ease of manipulation, but any model with an sklearn interface can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier on original data\n",
    "rf_model = RandomForestClassifier(n_estimators=25, \n",
    "                               max_depth=None,\n",
    "                               random_state=42).fit(x_train, y_train)\n",
    "\n",
    "rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting initial accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of our classifier: 0.8052005030857878\n",
      "Testing accuracy of our classifier: 0.8024295366136628\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "\n",
    "preds_test = rf_model.predict(x_test)\n",
    "\n",
    "acc_train = rf_model.score(x_train, y_train)\n",
    "acc_test = rf_model.score(x_test, y_test)\n",
    "\n",
    "print('Training accuracy of our classifier:',acc_train)\n",
    "print('Testing accuracy of our classifier:',acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on test should be around 80%. As we will see later, this value will decrease when we try to make our model fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Parity\n",
    "\n",
    "We will first test our model's predictions with statistical parity, a simple fairness measure that is easy to compute.\n",
    "\n",
    "### What is statistical parity?\n",
    "\n",
    "This metric measures the difference between the probability of positive decisions for the protected group and the probability of positive decisions for the unprotected group. Mathematically:\n",
    "$$Sp = P(d=1|G=0) - P(d=1|G=1)$$\n",
    "\n",
    "This can be easily approximated with our data by calculating the proportion of positive decisions amongst people from gender \"0\" and substracting the proportion of positive decisions amongst people from gender \"1\":\n",
    "\n",
    "$$Sp = \\frac{ \\text{# people with positive decision and gender 0}} { \\text{ # people from gender 0} } - \\frac{ \\text{# people with positive decision and gender 1}} { \\text{ # people from gender 1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code a simple function that will calculate this for our dataset. In the next cell, complete the function `evaluate_statistical_parity` to perform the calculation above. The function definition and docstring will guide you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical parity function\n",
    "def evaluate_statistical_parity(predictions, protected_class_array):\n",
    "    \"\"\"Function to calculate statistical parity.\n",
    "\n",
    "     Parameters\n",
    "    ----------\n",
    "    predictions (numpy array): binary decision labels outputted by our trained model.\n",
    "    protected_class_array (numpy array): boolean mask where protected rows are marked True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bias (float): statistical parity bias \n",
    "    (difference between proportion of positive decisions of protected class and unprotected class)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------\n",
    "    # --------------\n",
    "    # Your code here        \n",
    "    # --------------\n",
    "    # --------------\n",
    "\n",
    "    prop_protected = np.sum(predictions & protected_class_array) / np.sum(protected_class_array)\n",
    "    prop_not_protected = np.sum(predictions & ~protected_class_array) / np.sum(~protected_class_array)\n",
    "    bias = np.abs(prop_protected - prop_not_protected)\n",
    "\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing statistical parity\n",
    "\n",
    "We can now test this initial statistical measure and observe if our dataset is fair with respect to GENDER. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity on Test set: 0.22109782275375653\n"
     ]
    }
   ],
   "source": [
    "# Observing statistical parity on test set\n",
    "\n",
    "GENDER_COLUMN = 1\n",
    "\n",
    "predictions = rf_model.predict(x_test)>0.5\n",
    "protected_class_array = dataset_orig_test.features[:,GENDER_COLUMN]==0 # Here, we're taking the column corresponding to 'sex' and we are transforming it into a boolean array\n",
    "\n",
    "statistical_parity_orig = evaluate_statistical_parity(predictions, protected_class_array)\n",
    "\n",
    "print('Statistical Parity on Test set:', statistical_parity_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should get a value around 0.2 . Statistical parity shows perfect fairness when the indicator is 0. In this case, we do have a certain amount of unfairness: the proportion of positive classifications is not the same on both groups. \n",
    "\n",
    "This is interesting, but we're observing the protected group as a whole. What happens if we zoom in on a part of the dataset fulfilling a certain condition? Are we more or less fair?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Parity\n",
    "\n",
    "Statistical parity is a simple measure, and it gives a fast overview on our model's fairness. However, it disregards important aspects of our dataset, such as the values of the features of each row. Consider the loan application again. We could have a situation where the statistical parity measure tells us that we are giving loans to 20% of people from gender 0 and 20% of people from gender 1, which would be fair, but those 20% from gender 0 are chosen at random, while the 20% from gender 1 are all from developed countries. Our model would be hiding another layer of unfairness: we are not giving loans equally to people from gender 1.\n",
    "\n",
    "We can use conditional parity to detect these types of imbalances. Conditional parity allows us to test for unfairness in a similar way as Statistical Parity, but conditioning on another feature (for example, country of origin). The equation is:\n",
    "\n",
    "$$Cp = P(d=1|G=0, C=c) - P(d=1|G=1, C=c)$$\n",
    "\n",
    "Again, this can be easily calculated by counting the number of positive outcome cases in both protected groups, but this time only looking at the people that fulfill our conditional constraint (C=c). In simpler terms, the equation is:\n",
    "\n",
    "$$Cp = \\frac{ \\text{# people with positive decision and gender 0 and condition c}} { \\text{ # people from gender 0 and condition c} } - \\frac{ \\text{# people with positive decision and gender 1 and condition c}} { \\text{ # people from gender 1 and condition c}}$$\n",
    "\n",
    "We want to code this in the following function, `evaluate_conditional_parity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional parity function\n",
    "def evaluate_conditional_parity(predictions, protected_class_array, condition_array):\n",
    "        \"\"\"Function to calculate Conditional statistical parity.\n",
    "        \n",
    "         Parameters\n",
    "        ----------\n",
    "        predictions (numpy array): binary (decision) labels for X\n",
    "        protected_class_array (numpy array): boolean array where protected rows are marked True\n",
    "        condition_array (numpy array): boolean array that indicates conditional status\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bias (float): conditional parity bias\n",
    "        \"\"\"\n",
    "        \n",
    "        # --------------\n",
    "        # --------------\n",
    "        # Your code here\n",
    "        # --------------\n",
    "        # --------------\n",
    "        \n",
    "        \n",
    "        prop_protected = np.sum(predictions & condition_array & protected_class_array) / np.sum(condition_array & protected_class_array)\n",
    "        prop_not_protected = np.sum(predictions & condition_array & ~protected_class_array) / np.sum(condition_array & ~protected_class_array)\n",
    "        bias = np.abs(prop_protected - prop_not_protected)\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Conditional Parity\n",
    "We want to see what happens when we look at the subgroup corresponding to race=1. Are we being fair, in terms of statistical parity, in that subgroup? Our conditional vector is very simple in this case: it just corresponds to the `race` column in our dataset. Remember, however, that we're still looking at fairness in terms of gender; we're just observing a racial subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Parity on Test set, conditioned on people of race=1: 0.36339044183949504\n"
     ]
    }
   ],
   "source": [
    "# Observing statistical parity on test set\n",
    "\n",
    "GENDER_COLUMN = 1\n",
    "AGE_40_COLUMN = 5\n",
    "\n",
    "predictions = rf_model.predict(x_test)>0.5\n",
    "protected_class_array = dataset_orig_test.features[:,GENDER_COLUMN]==0 \n",
    "condition_array = dataset_orig_test.features[:,AGE_40_COLUMN]==1\n",
    "conditional_parity_orig = evaluate_conditional_parity(predictions, protected_class_array, condition_array)\n",
    "\n",
    "print('Conditional Parity on Test set, conditioned on people of race=1:', conditional_parity_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should get a conditional parity value of about 0.36. As we can see, we are actually being more unfair in the distribution of positive outcome for genders 0 and 1 in the subgroup corresponding to \"age decade 40\". This insight can be repeated with other subgroups of interest to assess the fairness of our algorithm in different cases.\n",
    "\n",
    "Up until now, we haven't looked at the validity of our predictions. What happens if we're interested in balancing the amount of *errors* that we make?\n",
    "\n",
    "The next metric will help us on that front."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Positive (Negative) Error Rate Balance\n",
    "\n",
    "The previous measures don't take into account the real labels of each observation; they only consider the predictions. The measure of fairness proposed here controls for equal poportions of false positives/false negatives in protected and unprotected classes. This measure is ideal in cases where committing mistakes disproportionately for different protected groups can bring negative outcomes.\n",
    "\n",
    "We will again code these measures as they are rather easy to understand. The function definition below will guide you through the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive and false negative rates\n",
    "def evaluate_false_negative_rate(predictions, protected, y):\n",
    "    \"\"\"evaluate fnr\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions (numpy array): binary (decision) labels for X predicted by our model\n",
    "    protected (numpy array): boolean mask where protected rows are marked True or 1\n",
    "    y (numpy array): boolean array that marks ground truth\n",
    "\n",
    "    Note:\n",
    "        FNR: FN / CP where FN=(predictions==0) & (y==1) CN = (y==1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bias (float)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------\n",
    "    # --------------\n",
    "    # Your code here\n",
    "    # --------------\n",
    "    # --------------\n",
    "    \n",
    "    cond_pos_protected = np.sum((y==1) & protected)\n",
    "    cond_pos_not_protected = np.sum((y==1) & ~protected)\n",
    "    \n",
    "    if cond_pos_protected == 0:\n",
    "        return 'No Condition Positive in Protected'\n",
    "    if cond_pos_not_protected == 0:\n",
    "        return 'No Condition Positive in Not Protected'\n",
    "\n",
    "    false_neg_protected = np.sum((y==1) & (predictions==0) & protected)\n",
    "    false_neg_not_protected = np.sum((y==1) & (predictions==0) & ~protected)\n",
    "\n",
    "    fnr_g = false_neg_protected / cond_pos_protected\n",
    "    fnr_not_g = false_neg_not_protected / cond_pos_not_protected\n",
    "    bias = np.abs(fnr_g - fnr_not_g)\n",
    "    \n",
    "    return bias\n",
    "\n",
    "\n",
    "def evaluate_false_positive_rate(predictions, protected, y):\n",
    "    \"\"\"evaluate fpr\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions (numpy array): binary (decision) labels for X predicted by our model\n",
    "    protected (numpy array): boolean mask where protected rows are marked True or 1\n",
    "    y (numpy array): boolean array that marks ground truth\n",
    "\n",
    "    Note:\n",
    "        FPR: FP / CN where FP=(predictions==1) & (y==0) CN = (y==0)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bias (float)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------\n",
    "    # --------------\n",
    "    # Your code here\n",
    "    # --------------\n",
    "    # --------------\n",
    "\n",
    "    cond_neg_protected = np.sum((y==0) & protected)\n",
    "    cond_neg_not_protected = np.sum((y==0) & ~protected)\n",
    "    \n",
    "    if cond_neg_protected == 0:\n",
    "        return 'No Condition Negative in Protected'\n",
    "    if cond_neg_not_protected == 0:\n",
    "        return 'No Condition Negative in Not Protected'\n",
    "\n",
    "    false_pos_protected = np.sum((y==0) & predictions & protected)\n",
    "    false_pos_not_protected = np.sum((y==0) & predictions & ~protected)\n",
    "\n",
    "    fpr_g = false_pos_protected / cond_neg_protected\n",
    "    fpr_not_g = false_pos_not_protected / cond_neg_not_protected\n",
    "    bias = np.abs(fpr_g - fpr_not_g)\n",
    "    return bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing FPR and FNR\n",
    "\n",
    "Again, we want these measures to be as close to zero as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate:  0.11588675370397536\n",
      "False negative rate: 0.46291301416048547\n"
     ]
    }
   ],
   "source": [
    "# Test FPR and FNR\n",
    "fpr = evaluate_false_positive_rate(predictions, protected_class_array, y_test)\n",
    "fnr = evaluate_false_negative_rate(predictions, protected_class_array, y_test)\n",
    "\n",
    "print('False positive rate: ',fpr)\n",
    "print('False negative rate:',fnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values of FPR and FNR are not as close to zero as we would want, showing at least a 10% disparity between the errors on protected classes vs the rest of the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Fairness metrics \n",
    "\n",
    "We have coded and tested some basic Fairness metrics, but there are multiple other metrics that can be used, depending on the situation. Some of them are:\n",
    "\n",
    "**Predictive parity:**\n",
    "The fraction of correct positive predictions should be the same for protected and unprotected groups.\n",
    "$$P(Y=1|d=1, G=m) = P(Y=1|d=1, G=f)$$\n",
    "\n",
    "\n",
    "**Equalized odds:**\n",
    "Applicants with a good actual credit scope and applicants with a bad actual credit\n",
    "score should have a similar classification, regardless of the value of the protected class.\n",
    "$$P(d=1|Y=i, G=m) = P(d=1|Y=i, G=f), i\\in \\{0,1\\}$$\n",
    "\n",
    "\n",
    "**Overall accuracy equality:**\n",
    "Both protected and unprotected groups have equal prediction accuracy.\n",
    "$$P(d=Y, G=m) = P(d=Y, G=f)$$\n",
    "\n",
    "\n",
    "**Treatment Equality:**\n",
    "Looks at ratio of errors a classifier makes instead of its accuracy. Satisfied if both protected and unprotected groups have equal ratio of false negatives and false positives.\n",
    "\n",
    "\n",
    "We will not go through all of them in code to save time. We will now try to generate a fair model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Fair Model\n",
    "\n",
    "Once we have characterized and measured the fairness of the model, we might want to build a model that avoids discrimination given a protected class. As there are multiple ways to define fairness, there are also multiple ways to build a fair classifier, depending on what notion we want to emphasize.\n",
    "\n",
    "Some options are:\n",
    "- Preprocessing the data to remove biases, and training normal classifiers on that data\n",
    "- Training the classifier and post-processing the predictions to accomodate our measures of fairness\n",
    "- Training a modified classifier with clear constraints that enforce fairness\n",
    "\n",
    "We will exemplify the Optimized Preprocessing technique, published by our very own Flavio Calmon.\n",
    "\n",
    "![../optimized.PNG]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "Let's take a look at the values we have so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of our classifier: 0.8052005030857878\n",
      "Testing accuracy of our classifier: 0.8024295366136628\n",
      "Statistical Parity on Test set: 0.22109782275375653\n",
      "Conditional Parity on Test set, conditioned on people of race=1: 0.36339044183949504\n",
      "False positive rate on Test set:  0.11588675370397536\n",
      "False negative rate on Test set: 0.46291301416048547\n"
     ]
    }
   ],
   "source": [
    "# Getting accuracy and fairness metrics on test\n",
    "\n",
    "print('Training accuracy of our classifier:',acc_train)\n",
    "print('Testing accuracy of our classifier:',acc_test)\n",
    "print('Statistical Parity on Test set:', statistical_parity_orig)\n",
    "print('Conditional Parity on Test set, conditioned on people of race=1:', conditional_parity_orig)\n",
    "print('False positive rate on Test set: ',fpr)\n",
    "print('False negative rate on Test set:',fnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's apply a dataset transformation to increase fairness !\n",
    "\n",
    "Using AIF360 OptimPreproc module, we can transform our dataset into a new representation that will improve our metrics above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Camilo\\Anaconda3\\lib\\site-packages\\aif360\\algorithms\\preprocessing\\optim_preproc.py:73: UserWarning: Privileged and unprivileged groups specified will not be used. The protected attributes are directly specified in the data preprocessing function. The current implementation automatically adjusts for discrimination across all groups. This can be changed by changing the optimization code.\n",
      "  warn(\"Privileged and unprivileged groups specified will not be \"\n",
      "C:\\Users\\Camilo\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py:661: RuntimeWarning: overflow encountered in long_scalars\n",
      "  if self.max_big_small_squared < big*small**2:\n",
      "C:\\Users\\Camilo\\Anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py:662: RuntimeWarning: overflow encountered in long_scalars\n",
      "  self.max_big_small_squared = big*small**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Preprocessing: Objective converged to 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Instantiate OptimizedDataPreprocessing module from AIF360    \n",
    "OP = OptimPreproc(OptTools, optim_options,\n",
    "                  unprivileged_groups = unprivileged_groups,\n",
    "                  privileged_groups = privileged_groups)\n",
    "\n",
    "# Fit the module to the training data, effectively creating the mapping from original data to transformed, fair data\n",
    "OP = OP.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training data and align features\n",
    "dataset_transf_train = OP.transform(dataset_orig_train, transform_Y=True)\n",
    "dataset_transf_train = dataset_orig_train.align_datasets(dataset_transf_train)\n",
    "\n",
    "# Same with test data\n",
    "dataset_transf_test = OP.transform(dataset_orig_test, transform_Y = True)\n",
    "dataset_transf_test = dataset_orig_test.align_datasets(dataset_transf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we have to get our training numpy arrays, this time on the TRANSFORMED training data\n",
    "x_train_transf = scale_orig.fit_transform(dataset_transf_train.features)\n",
    "y_train_transf = (dataset_transf_train.labels.ravel()-2)*-1\n",
    "y_train_transf = dataset_transf_train.labels.ravel()\n",
    "\n",
    "# And our testing arrays, on the TRANSFORMED test data\n",
    "x_test_transf = scale_orig.transform(dataset_transf_test.features) # Here, we only transform, as we can't use the testing set to define the scaling factors.\n",
    "y_test_transf = (dataset_transf_test.labels.ravel()-2)*-1\n",
    "y_test_transf = dataset_transf_test.labels.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier on TRANSFORMED data\n",
    "rf_model_transf = RandomForestClassifier(n_estimators=25, \n",
    "                               max_depth=None,\n",
    "                               random_state=42).fit(x_train_transf, y_train_transf)\n",
    "rf_model_transf.fit(x_train_transf, y_train_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test with original data (we should expect a bit less than before): 0.756159148297277\n"
     ]
    }
   ],
   "source": [
    "# Getting accuracy and fairness metrics on TRANSFORMED test set\n",
    "acc_transf_train = rf_model_transf.score(x_train_transf, y_train_transf)\n",
    "acc_transf_test = rf_model_transf.score(x_test_transf, y_test_transf)\n",
    "print('Accuracy on test with original data (we should expect a bit less than before):', acc_transf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting NEW fairness metrics\n",
    "\n",
    "predictions_transf = rf_model_transf.predict(x_test_transf)>0.5\n",
    "protected_class_array_transf = dataset_orig_test.features[:,GENDER_COLUMN]==0 \n",
    "\n",
    "statistical_parity_transf = evaluate_statistical_parity(predictions_transf, protected_class_array_transf)\n",
    "conditional_parity_transf = evaluate_conditional_parity(predictions_transf, protected_class_array_transf, condition_array)\n",
    "fpr_transf = evaluate_false_positive_rate(predictions_transf, protected_class_array_transf, y_test_transf)\n",
    "fnr_transf = evaluate_false_negative_rate(predictions_transf, protected_class_array_transf, y_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATA\n",
      "Training accuracy of our classifier: 0.8052005030857878\n",
      "Testing accuracy of our classifier: 0.8024295366136628\n",
      "Statistical Parity on Test set: 0.22109782275375653\n",
      "Conditional Parity on Test set, conditioned on people of race=1: 0.36339044183949504\n",
      "False positive rate on Test set:  0.11588675370397536\n",
      "False negative rate on Test set: 0.46291301416048547\n",
      "\n",
      "\n",
      "TRANSFORMED DATA\n",
      "Training accuracy of our classifier: 0.7580508350639095\n",
      "Testing accuracy of our classifier: 0.756159148297277\n",
      "Statistical Parity on Test set: 0.06649123768108822\n",
      "Conditional Parity on Test set, conditioned on people of race=1: 0.2892247671857212\n",
      "False positive rate on Test set:  0.01984584939953457\n",
      "False negative rate on Test set: 0.1610829049999274\n"
     ]
    }
   ],
   "source": [
    "# COmparing metrics\n",
    "print('ORIGINAL DATA')\n",
    "print('Training accuracy of our classifier:',acc_train)\n",
    "print('Testing accuracy of our classifier:',acc_test)\n",
    "print('Statistical Parity on Test set:', statistical_parity_orig)\n",
    "print('Conditional Parity on Test set, conditioned on people of race=1:', conditional_parity_orig)\n",
    "print('False positive rate on Test set: ',fpr)\n",
    "print('False negative rate on Test set:',fnr)\n",
    "\n",
    "\n",
    "print('\\n\\nTRANSFORMED DATA')\n",
    "print('Training accuracy of our classifier:',acc_transf_train)\n",
    "print('Testing accuracy of our classifier:',acc_transf_test)\n",
    "print('Statistical Parity on Test set:', statistical_parity_transf)\n",
    "print('Conditional Parity on Test set, conditioned on people of race=1:', conditional_parity_transf)\n",
    "print('False positive rate on Test set: ',fpr_transf)\n",
    "print('False negative rate on Test set:',fnr_transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our Fairness metrics improved in all cases, and our accuracy took a hit, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We have analyzed particular fairness metrics and observed their behavior on an artificial dataset. It is important to remember that Fairness has multiple definitions, each one approriate for analyzing a specific situation. Statistical notions of fairness as described above are easy to measure. However, it is important to keep in mind that statistical definitions are insufficient in some cases (for example, when similarity has to be taken into account). Moreover, most valuable statistical metrics assume availability of actual, verified outcomes. While such outcomes are available for the training data, it is unclear whether the real classified data always conforms to the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: extra resources\n",
    "\n",
    "## Interesting Fairness analysis tools\n",
    "- Pymetrics audit-ai (https://github.com/pymetrics/audit-ai)\n",
    "- fairness metrics github (https://github.com/megantosh/fairness_measures_code)\n",
    "- fairness-comparison github (https://github.com/algofairness/fairness-comparison)\n",
    "- IBM AIF360 (https://github.com/IBM/AIF360, https://arxiv.org/pdf/1810.01943.pdf)\n",
    "- Themis ML (https://themis-ml.readthedocs.io/en/latest/)\n",
    "- FairML (https://github.com/adebayoj/fairml)\n",
    "- BlackBoxAuditing (https://github.com/algofairness/BlackBoxAuditing)\n",
    "\n",
    "## Interesting papers\n",
    "- Learning Fair Representations (seminal paper) http://proceedings.mlr.press/v28/zemel13.pdf\n",
    "- Optimized Data Pre-Processing for Discrimination Prevention (by Flavio Calmon) https://arxiv.org/pdf/1704.03354.pdf\n",
    "- Fairness Definitions Explained http://fairware.cs.umass.edu/papers/Verma.pdf\n",
    "- From parity to Preference-based notions of fairness https://arxiv.org/abs/1707.00010\n",
    "- Certifying and removing disparate impact https://arxiv.org/pdf/1412.3756.pdf\n",
    "- Learning Classification without Disparate Mistreatment https://arxiv.org/pdf/1610.08452.pdf\n",
    "- Fairness Constraints: Mechanisms for Fair Classification https://arxiv.org/abs/1507.05259\n",
    "- Fairness GAN https://arxiv.org/pdf/1805.09910.pdf\n",
    "- Adversarial Debiasing https://arxiv.org/pdf/1801.07593.pdf\n",
    "- Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees https://arxiv.org/pdf/1806.06055.pdf\n",
    "\n",
    "\n",
    "## Fairness concepts\n",
    "- **Fairness through unawareness:**\n",
    "No sensitive attributes used in the decision making process.\n",
    "- **Fairness through awareness:**\n",
    "Similar individuals should have similar classification.\n",
    "- **Disparate impact:**\n",
    "Exists when decision outcomes disproportionately benefits or hurts individuals of a certain group.\n",
    "- **Disparate treatment:**\n",
    "Decision changes when protected feature changes.\n",
    "- **Disparate mistreatment:**\n",
    "Missclassification rates are different for people of different protected groups\n",
    "\n",
    "\n",
    "We refer the reader to http://fairware.cs.umass.edu/papers/Verma.pdf for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
